{
  "logging": {
    "log_file": "logs/smoke_pipeline.log",
    "trace_workflow_name": "gpt5-nano-smoke",
    "trace_metadata": {
      "project": "agent-pipeline-smoke",
      "benchmark": "gpt5-nano"
    }
  },
  "io": {
    "input_artifact": "artifacts/lark_docs.json",
    "output_directory": "artifacts/smoke_output"
  },
  "concurrency": {
    "router_pool": 12,
    "markdown_pool": 16,
    "notebook_pool": 8,
    "review_pool": 6,
    "naming_pool": 4,
    "channel_buffer": 64,
    "buffers": {
      "router": 32,
      "markdown": 48,
      "notebook": 32,
      "review": 32,
      "naming": 16,
      "write": 16
    },
    "model_limits": {
      "gpt-5-nano": 32
    },
    "agent_thread_limit": 48
  },
  "retry": {
    "max_attempts": 5,
    "initial_delay": 0.5,
    "backoff_multiplier": 2.0,
    "max_delay": 60.0,
    "jitter_ratio": 0.25
  },
  "httpx": {
    "connect": 5.0,
    "read": null,
    "write": 30.0,
    "pool": 5.0
  },
  "agents": {
    "router": {
      "name": "Router",
      "instructions": "Classify incoming documentation as markdown prose or notebook content. Respond with the route only.",
      "model": "gpt-5-nano",
      "reasoning_effort": "minimal",
      "verbosity": "low",
      "timeout": 180.0,
      "max_tokens": 128
    },
    "markdown_cleaner": {
      "name": "MarkdownPolisher",
      "instructions": "Normalize markdown documentation while preserving technical accuracy and code fences.",
      "model": "gpt-5-nano",
      "reasoning_effort": "minimal",
      "verbosity": "low",
      "timeout": 240.0,
      "max_tokens": 2048
    },
    "notebook_refactor": {
      "name": "NotebookRefactorer",
      "instructions": "Convert notebook content into a concise Python script with comments and preserved grammar blocks.",
      "model": "gpt-5-nano",
      "reasoning_effort": "minimal",
      "verbosity": "low",
      "timeout": 240.0,
      "max_tokens": 2048
    },
    "reviewer": {
      "name": "QualityReviewer",
      "instructions": "Approve only if the transformed document is correct, consistent, and ready to publish. Otherwise, provide actionable issues.",
      "model": "gpt-5-nano",
      "reasoning_effort": "minimal",
      "verbosity": "low",
      "timeout": 180.0,
      "max_tokens": 512
    },
    "namer": {
      "name": "FileNamer",
      "instructions": "Produce a kebab-case slug and extension appropriate for the processed document.",
      "model": "gpt-5-nano",
      "reasoning_effort": "minimal",
      "verbosity": "low",
      "timeout": 120.0,
      "max_tokens": 128
    }
  }
}
